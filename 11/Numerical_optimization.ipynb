{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will learn to solve non-convex multi-dimensional optimization problems using numerical optimization with multistart and nesting (**scipy.optimize**). You will learn simple function approximation using linear interpolation (**scipy.interp**). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Links:**\n",
    "\n",
    "1. **scipy.optimize:** [overview](https://docs.scipy.org/doc/scipy/reference/optimize.html) + [tutorial](https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html)\n",
    "2. **scipy.interp:** [overview](https://docs.scipy.org/doc/scipy/reference/interpolate.html) + [tutorial](https://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful note:** [Numerical Optimization in MATLAB](http://www.google.com/url?q=http%3A%2F%2Fweb.econ.ku.dk%2Fmunk-nielsen%2Fnotes%2FnoteOptimization.pdf&sa=D&sntz=1&usg=AFQjCNHX4tHx2_YsNaIt5FB5MBU5cfcS8g) (by Anders Munk-Nielsen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import linalg\n",
    "from scipy import optimize\n",
    "from scipy import interpolate\n",
    "import sympy as sm\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All **optimization problems** are characterized by:\n",
    "\n",
    "1. Control vector (choices), $\\boldsymbol{x} \\in \\mathbb{R}^k$\n",
    "2. Objective function (payoff) to minimize, $f:\\mathbb{R}^k \\rightarrow \\mathbb{R}$ (differentiable or not)\n",
    "3. Constraints, i.e. $\\boldsymbol{x}  \\in C \\subseteq \\mathbb{R}^k$ (linear or non-linear interdependence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $f$ might also take other inputs (parameters or a dataset), but these are fixed, and therefore not variables we optimize over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maximization** is just **minimization** of $-f$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All **optimizers** (minimizers) follow the structure:\n",
    "\n",
    "1. Make initial guess\n",
    "2. Evaluate the function (and perhaps gradients)\n",
    "3. Check for convergence\n",
    "4. Update guess and return to step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convergence:** \"Small\" change in function value since last iteration (or \"zero\" gradient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Characteristics** of optimizers:\n",
    "\n",
    "1. Use gradients or not.\n",
    "2. Allow for specifying bounds.\n",
    "3. Allow for specifying general constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradients** provide useful information, but can be costly to compute (using analytical formula or numerically)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penalty terms** can (sometimes) instead be used to enforce bounds and constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizers** you should know:\n",
    "\n",
    "1. **Nelder-Mead:** \n",
    " * **Pro:** Robust (to e.g. noise in objective function) and does not require derivatives.\n",
    " * **Con:** Slow convergence. No bounds or constraints.\n",
    "2. **Newton-CG:**\n",
    " * **Pro:** Require few iterations. Very precise with analytical hessian for smooth functions.\n",
    " * **Con:** Costly computation of hessian. No bounds or constraints.\n",
    "3. **BFGS:** (like newton, but with smart computation of hessian)\n",
    "  * **Pro:** Require few function evaluations. \n",
    "  * **Con:** No bounds or constraints.\n",
    "4. **L-BFGS-B:** Like BFGS, but allows for bounds.\n",
    "5. **SLSQP:**\n",
    "  * **Pro:** Bounds and constraints in multiple dimensions.\n",
    "  * **Con:** Not as efficient as BFGS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient based optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the idea behind gradient based optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One dimensional intuition:** Consider the second-order Taylor approximation around $x_n$:\n",
    "\n",
    "$$ \n",
    "f_T(x) = f_T(x_n + \\Delta x) \\approx f(x_n)+ f^{\\prime}(x_n) \\Delta x + \\frac{1}{2} f^{\\prime\\prime}(x_n) (\\Delta x)^2\n",
    "$$\n",
    "\n",
    "Find the minimum wrt. to $\\Delta x$ by solving the FOC:\n",
    "\n",
    "$$\n",
    "0 = \\frac{d}{d\\Delta x} f_T(x) = f^{\\prime}(x_n) + f^{\\prime\\prime}(x_n) \\Delta x \\Leftrightarrow \\Delta x = -\\frac{f^{\\prime}(x_n)}{f^{\\prime\\prime}(x_n)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm:** `minimize_newton()`\n",
    "\n",
    "1. Choose tolerance $\\epsilon>0$, guess on $\\boldsymbol{x}_0$, compute $f(\\boldsymbol{x}_0)$, and set $n=1$.\n",
    "2. Compute $\\nabla f(\\boldsymbol{x}_{n-1})$ (gradient/jacobian) and $\\boldsymbol{H}f(\\boldsymbol{x}_{n-1})$ (hessian).\n",
    "3. Compute new guess\n",
    "\n",
    "  $$ \n",
    "  \\boldsymbol{x}_{n} = \\boldsymbol{x}_{n-1} - [\\boldsymbol{H}f(\\boldsymbol{x}_{n-1})]^{-1} \\nabla f(\\boldsymbol{x}_{n-1})\n",
    "  $$\n",
    "\n",
    "3. If $|f(\\boldsymbol{x}_n)-f(\\boldsymbol{x}_{n-1})| < \\epsilon$ then stop.\n",
    "5. Set $n = n + 1$ and return to step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_newton(f,x0,jac,hess,max_iter=500,tol=1e-8):\n",
    "    \"\"\" minimize function with Newtons' algorithm\n",
    "        \n",
    "    Args:\n",
    "\n",
    "        f (callable): function\n",
    "        x0 (np.ndarray): initial values\n",
    "        jac (callable): jacobian\n",
    "        hess (callable): hessian\n",
    "        max_iter (int): maximum number of iterations\n",
    "        tol (float): tolerance\n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "        x (np.ndarray): minimum\n",
    "        n (int): number of iterations used\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # step 1: initialize\n",
    "    x = x0\n",
    "    fx = f(x0)\n",
    "    n = 1\n",
    "    \n",
    "    # step 2-5: iteration\n",
    "    while n < max_iter:\n",
    "        \n",
    "        x_prev = x\n",
    "        fx_prev = fx\n",
    "        \n",
    "        # step 2: evaluate gradient and hessian\n",
    "        jacx = jac(x_prev)\n",
    "        hessx = hess(x_prev)\n",
    "        \n",
    "        # step 3: update x\n",
    "        inv_hessx = linalg.inv(hessx)        \n",
    "        x = x_prev - inv_hessx@jacx\n",
    "     \n",
    "        # step 4: check convergence\n",
    "        fx = f(x)\n",
    "        if abs(fx-fx_prev) < tol:\n",
    "            break\n",
    "            \n",
    "        # step 5: increment n\n",
    "        n += 1\n",
    "        \n",
    "    return x,n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm:** `minimize_gradient_descent()`\n",
    "\n",
    "1. Choose tolerance $\\epsilon>0$, potential step sizes, $ \\boldsymbol{\\alpha} = [\\alpha_0,\\alpha_1,\\dots,\\alpha_\\#]$, guess on $\\boldsymbol{x}_0$, compute $f(\\boldsymbol{x}_0)$, and set $n=1$.\n",
    "2. Compute $\\nabla f(\\boldsymbol{x}_{n-1})$.\n",
    "3. Find good step size:\n",
    "\n",
    "  $$ \n",
    "  \\alpha^{\\ast} = \\arg \\min_{\\alpha \\in \\boldsymbol{\\alpha}}  f(\\boldsymbol{x}_{n-1} - \\alpha \\nabla f(\\boldsymbol{x}_{n-1}))\n",
    "  $$\n",
    "\n",
    "4. Compute new guess:\n",
    "\n",
    "  $$\n",
    "  \\boldsymbol{x}_{n} = \\boldsymbol{x}_{n-1} - \\alpha^{\\ast} \\nabla f(\\boldsymbol{x}_{n-1})\n",
    "  $$\n",
    "\n",
    "5. If $|f(\\boldsymbol{x}_n)-f(\\boldsymbol{x}_{n-1})| < \\epsilon$ then stop.\n",
    "6. Set $n = n + 1$ and return to step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_gradient_descent(f,x0,jac,alphas=[0.01,0.05,0.1,0.25,0.5,1],max_iter=500,tol=1e-8):\n",
    "    \"\"\" minimize function with gradient descent\n",
    "        \n",
    "    Args:\n",
    "\n",
    "        f (callable): function\n",
    "        x0 (np.ndarray): initial values\n",
    "        jac (callable): jacobian\n",
    "        alpha (list): potential step sizes\n",
    "        max_iter (int): maximum number of iterations\n",
    "        tol (float): tolerance\n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "        x (np.ndarray): minimum\n",
    "        n (int): number of iterations used\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # step 1: initialize\n",
    "    x = x0\n",
    "    fx = f(x0)\n",
    "    n = 1\n",
    "    \n",
    "    # step 2-6: iteration\n",
    "    while n < max_iter:\n",
    "            \n",
    "        x_prev = x\n",
    "        fx_prev = fx\n",
    "        \n",
    "        # step 2: evaluate gradient\n",
    "        jacx = jac(x)\n",
    "        \n",
    "        # step 3: find good step size (line search)\n",
    "        fx_ast = np.inf\n",
    "        alpha_ast = np.nan\n",
    "        for alpha in alphas:\n",
    "            x = x_prev - alpha*jacx\n",
    "            fx = f(x)\n",
    "            if fx < fx_ast:\n",
    "                fx_ast = fx\n",
    "                alpha_ast = alpha\n",
    "        \n",
    "        # step 4: update guess\n",
    "        x = x_prev - alpha_ast*jacx\n",
    "                            \n",
    "        # step 5: check convergence\n",
    "        fx = f(x)\n",
    "        if abs(fx-fx_prev) < tol:\n",
    "            break\n",
    "            \n",
    "        # d. update i\n",
    "        n += 1\n",
    "        \n",
    "    return x,n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Many generalizations:**\n",
    "\n",
    "1. Use both Hessian and line search\n",
    "2. Stop line search when improvement found\n",
    "3. Limit attention to a \"trust-region\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "etc. etc. etc. etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: The rosenbrock function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the **rosenbrock function**:\n",
    "\n",
    "$$ \n",
    "f(\\boldsymbol{x}) = f(x_1,x_2) =0.5(1-x_{1})^{2}+(x_{2}-x_{1}^{2})^{2}\n",
    "$$\n",
    "\n",
    "with **jacobian** (gradient)\n",
    "\n",
    "$$ \n",
    "\\nabla f(\\boldsymbol{x})=\\begin{bmatrix}\\frac{\\partial f}{\\partial x_{1}}\\\\\n",
    "\\frac{\\partial f}{\\partial x_{2}}\n",
    "\\end{bmatrix}=\\begin{bmatrix}-(1-x_{1})-4x_{1}(x_{2}-x_{1}^{2})\\\\\n",
    "2(x_{2}-x_{1}^{2})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and **hessian**:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{H}f(\\boldsymbol{x})=\\begin{bmatrix}\\frac{\\partial f}{\\partial x_{1}x_{1}} & \\frac{\\partial f}{\\partial x_{1}x_{2}}\\\\\n",
    "\\frac{\\partial f}{\\partial x_{1}x_{2}} & \\frac{\\partial f}{\\partial x_{2}x_{2}}\n",
    "\\end{bmatrix}=\\begin{bmatrix}1-4x_{2}+12x_{1}^{2} & -4x_{1}\\\\\n",
    "-4x_{1} & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Note:** Minimum is at $(1,1)$ where $f(1,1)=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check jacobian and hessian:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = sm.symbols('x_1')\n",
    "x2 = sm.symbols('x_2')\n",
    "f = 0.5*(1.0-x1)**2 + (x2-x1**2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df = sm.Matrix([sm.diff(f,i) for i in [x1,x2]])\n",
    "Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hf = sm.Matrix([[sm.diff(f,i,j) for j in [x1,x2]] for i in [x1,x2]])\n",
    "Hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rosen(x1,x2):\n",
    "    return 0.5*(1.0-x1)**2+(x2-x1**2)**2\n",
    "def rosen(x):\n",
    "    return _rosen(x[0],x[1])\n",
    "def rosen_jac(x):\n",
    "    return np.array([-(1.0-x[0])-4*x[0]*(x[1]-x[0]**2),2*(x[1]-x[0]**2)])\n",
    "def rosen_hess(x):\n",
    "    return np.array([[1-4*x[1]+12*x[0]**2,-4*x[0]],[-4*x[0],2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3D Plot:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. grids\n",
    "x1_vec = np.linspace(-2,2,500)\n",
    "x2_vec = np.linspace(-2,2,500)\n",
    "x1_grid,x2_grid = np.meshgrid(x1_vec,x2_vec,indexing='ij')\n",
    "rosen_grid = _rosen(x1_grid,x2_grid)\n",
    "\n",
    "# b. main\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1,projection='3d')\n",
    "cs = ax.plot_surface(x1_grid,x2_grid,rosen_grid,cmap=cm.jet)\n",
    "\n",
    "# c. add labels\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_zlabel('$u$')\n",
    "\n",
    "# d. invert xaxis\n",
    "ax.invert_xaxis()\n",
    "\n",
    "# e. add colorbar\n",
    "fig.colorbar(cs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contour plot:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "levels = [1e-6,5*1e-6,1e-5,5*1e-5,1e-4,5*1e-4,1e-3,5*1e-3,1e-2,5*1e-2,1,2,4,6,8,12,16,20]\n",
    "cs = ax.contour(x1_grid,x2_grid,rosen_grid,levels=levels,cmap=cm.jet)\n",
    "fig.colorbar(cs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Newton:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([5,4])\n",
    "x,n = minimize_newton(rosen,x0,rosen_jac,rosen_hess)\n",
    "print(n,x,rosen(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient descent:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([5,4])\n",
    "x,n = minimize_gradient_descent(rosen,x0,rosen_jac,alphas=[0.01,0.05,0.1,0.25,0.5,1])\n",
    "print(n,x,rosen(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:** Any ideas for getting the gradient descent optimizer to converge faster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy minimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preperation I:** Function for collecting infomation while running optimizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complicated -> not necessary to understand it\n",
    "def collect(x):\n",
    "    \n",
    "    # globals used to keep track across iterations\n",
    "    global evals # set evals = 0 before calling optimizer\n",
    "    global x0\n",
    "    global x1s\n",
    "    global x2s\n",
    "    global fs\n",
    "    \n",
    "    # a. initialize list\n",
    "    if evals == 0:\n",
    "        x1s = [x0[0]] \n",
    "        x2s = [x0[1]]\n",
    "        fs = [rosen(x0)]\n",
    "        \n",
    "    # b. append trial values\n",
    "    x1s.append(x[0])\n",
    "    x2s.append(x[1])\n",
    "    fs.append(rosen(x))\n",
    "    \n",
    "    # c. increment number of evaluations\n",
    "    evals += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preperation II:** Function plotting the collected information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complicated -> not necessary to understand it\n",
    "def contour():\n",
    "    \n",
    "    global evals\n",
    "    global x1s\n",
    "    global x2s\n",
    "    global fs\n",
    "    \n",
    "    # a. contour plot\n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    ax = fig.add_subplot(1,2,1)\n",
    "    levels = [1e-6,5*1e-6,1e-5,5*1e-5,1e-4,5*1e-4,1e-3,5*1e-3,1e-2,5*1e-2,1,2,4,6,8,12,16,20]\n",
    "    cs = ax.contour(x1_grid,x2_grid,rosen_grid,levels=levels,cmap=cm.jet)\n",
    "    fig.colorbar(cs)\n",
    "    ax.plot(x1s,x2s,'-o',ms=4,color='black')\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    \n",
    "    # b. function value\n",
    "    ax = fig.add_subplot(1,2,2)\n",
    "    ax.plot(np.arange(evals+1),fs,'-o',ms=4,color='black')\n",
    "    ax.set_xlabel('iteration')\n",
    "    ax.set_ylabel('function value')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nelder-Mead**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = 0 # global used in \"collect\"\n",
    "x0 = [-1.5,-1]\n",
    "result = optimize.minimize(rosen,x0,\n",
    "                           method='Nelder-Mead',\n",
    "                           callback=collect, # call collect() before each iteration\n",
    "                           options={'disp':True}) # display the results\n",
    "contour()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Does not require a gradient. Slow convergence close to target.\n",
    ">\n",
    "> **Iterations:** How many steps the algorithm has taken.\n",
    ">\n",
    "> **Function evaluations:** Will be higher than iterations. Used to compute next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also **print the information on results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also acess specific information of the result object: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.nit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Newton** (with analytical hessian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = 0 # global used in \"collect\"\n",
    "x0 = [-1.5,-1]\n",
    "result = optimize.minimize(rosen,x0,jac=rosen_jac,hess=rosen_hess,\n",
    "                           method='Newton-CG',\n",
    "                           callback=collect,\n",
    "                           options={'disp':True})\n",
    "contour()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Smoother and faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Newton** (with numerical hessian computed by scipy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = 0 # global used in \"collect\"\n",
    "x0 = [-1.5,-1]\n",
    "result = optimize.minimize(rosen,x0,jac=rosen_jac,\n",
    "                           method='Newton-CG',\n",
    "                           callback=collect,\n",
    "                           options={'disp':True})\n",
    "contour()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Same as above, but gradient evaluations instead of hessian evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BFGS** (with analytical gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = 0 # global used in \"collect\"\n",
    "x0 = [-1.5,-1]\n",
    "result = optimize.minimize(rosen,x0,jac=rosen_jac,\n",
    "                           method='BFGS',\n",
    "                           callback=collect,\n",
    "                           options={'disp':True})\n",
    "contour()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Non-smooth, but fast. Very low number of function evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BFGS** (with numerical gradient computed by scipy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = 0 # global used in \"collect\"\n",
    "x0 = [-1.5,-1]\n",
    "result = optimize.minimize(rosen,x0, # no jac= specified\n",
    "                           method='BFGS',\n",
    "                           callback=collect,\n",
    "                           options={'disp':True})\n",
    "contour()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Same as above, but more function evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L-BFGS-B** (with analytical gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = 0 # global used in \"collect\"\n",
    "x0 = [-1.5,-1]\n",
    "result = optimize.minimize(rosen,x0,jac=rosen_jac,\n",
    "                           method='L-BFGS-B',\n",
    "                           bounds=((-3,3),(-3,3)),\n",
    "                           callback=collect,\n",
    "                           options={'disp':True})\n",
    "contour()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SLSQP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = 0 # global used in \"collect\"\n",
    "x0 = [-1.5,-1]\n",
    "result = optimize.minimize(rosen,x0,jac=rosen_jac,\n",
    "                           method='SLSQP',\n",
    "                           bounds=((-2,2),(-2,2)),\n",
    "                           callback=collect,\n",
    "                           options={'disp':True})\n",
    "contour()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controling the optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** See the settings for each optimizer in the [documention](https://docs.scipy.org/doc/scipy/reference/optimize.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can lower the **tolerance**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = 0\n",
    "x0 = [-1.5,-1]\n",
    "result = optimize.minimize(rosen,x0,\n",
    "                           method='BFGS',\n",
    "                           callback=collect,\n",
    "                           options={'disp':True,'gtol':1e-8}) # note this\n",
    "contour()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the **maximum number of iterations**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = 0\n",
    "x0 = [-1.5,-1]\n",
    "result = optimize.minimize(rosen,x0,\n",
    "                           method='BFGS',\n",
    "                           callback=collect,\n",
    "                           options={'disp':True,'maxiter':5}) # note this and warning\n",
    "contour()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Can we make the program stop if the maximum number of iterations is too low?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sombrero function: Local minima and multistart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the **sombrero** function\n",
    "\n",
    "$$\n",
    "f(x_1,x_2) = g\\Big(\\sqrt{x_1^2 + x_2^2}\\Big)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "g(r) = -\\frac{\\sin(r)}{r+10^{-4}} + 10^{-4}r^2\n",
    "$$\n",
    "\n",
    "The **global minimum** of this function is (0,0). But the function also have (infinitely many) **local minima**. How to avoid these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sombrero(x1,x2):\n",
    "    r = np.sqrt(x1**2 + x2**2)\n",
    "    return -np.sin(r)/(r+1e-4) + 1e-4*r**2\n",
    "    \n",
    "sombrero = lambda x: _sombrero(x[0],x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. grids\n",
    "x1_vec = np.linspace(-15,15,500)\n",
    "x2_vec = np.linspace(-15,15,500)\n",
    "x1_grid_sombrero,x2_grid_sombrero = np.meshgrid(x1_vec,x2_vec,indexing='ij')\n",
    "sombrero_grid = _sombrero(x1_grid_sombrero,x2_grid_sombrero)\n",
    "\n",
    "# b. main\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1,projection='3d')\n",
    "cs = ax.plot_surface(x1_grid_sombrero,x2_grid_sombrero,sombrero_grid,cmap=cm.jet)\n",
    "\n",
    "# c. add labels\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_zlabel('$u$')\n",
    "\n",
    "# d. invert xaxis\n",
    "ax.invert_xaxis()\n",
    "\n",
    "# e. colorbar\n",
    "fig.colorbar(cs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-start - BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-start:** Draw many random starting values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1986)\n",
    "x0s = -15 + 30*np.random.uniform(size=(5000,2)) # in [-15,15]\n",
    "xs = np.empty((5000,2))\n",
    "fs = np.empty(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to solve with **BFGS** starting from each of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fopt = np.inf\n",
    "xopt = np.nan\n",
    "for i,x0 in enumerate(x0s):\n",
    "    \n",
    "    # a. optimize\n",
    "    result = optimize.minimize(sombrero,x0,method='BFGS')\n",
    "    xs[i,:] = result.x\n",
    "    f = result.fun\n",
    "    \n",
    "    # b. print first 10 or if better than seen yet\n",
    "    if i < 10 or f < fopt: # plot 10 first or if improving\n",
    "        if f < fopt:\n",
    "            fopt = f\n",
    "            xopt = xs[i,:]\n",
    "            \n",
    "        print(f'{i:4d}: x0 = ({x0[0]:6.2f},{x0[1]:6.2f})',end='')\n",
    "        print(f' -> converged at ({xs[i][0]:6.2f},{xs[i][1]:6.2f}) with f = {f:.14f}')\n",
    "        \n",
    "# best solution\n",
    "print(f'\\nbest solution:\\n x = ({xopt[0]:6.2f},{xopt[1]:6.2f}) -> f = {fopt:.14f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solver, wrongly, **converges to many of the local minima**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.scatter(xs[:,0],xs[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-start - Nelder-Mead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to solve with **Nelder-Mead** starting from each of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fopt = np.inf\n",
    "xopt = np.nan\n",
    "for i,x0 in enumerate(x0s):\n",
    "    \n",
    "    # a. optimize\n",
    "    result = optimize.minimize(sombrero,x0,method='Nelder-Mead')\n",
    "    xs[i,:] = result.x\n",
    "    f = result.fun\n",
    "    \n",
    "    # b. print first 10 or if better than seen yet\n",
    "    if i < 10 or f < fopt: # plot 10 first or if improving\n",
    "        if f < fopt:\n",
    "            fopt = f\n",
    "            xopt = xs[i,:]\n",
    "        print(f'{i:4d}: x0 = ({x0[0]:6.2f},{x0[1]:6.2f})',end='')\n",
    "        print(f' -> converged at ({xs[i][0]:6.2f},{xs[i][1]:6.2f}) with f = {f:.12f}')\n",
    "\n",
    "# best solution\n",
    "print(f'\\nbest solution:\\n x = ({xopt[0]:6.2f},{xopt[1]:6.2f}) -> f = {fopt:.12f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.scatter(xs[:,0],xs[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is there a better solution than multi-start?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In short:** No.\n",
    "\n",
    "**Potential improvement:** Use information from previous run to determine, where to look next. Fundamental trade-off between:\n",
    "\n",
    "1. **Exploitation.** Focus on areas where previous evaluations returned low function values.\n",
    "2. **Exploration.** Focus on completely new areas. \n",
    "\n",
    "**Heuristic:** If the same optimum is obtained for many starting values, this is a good sign for it being the global optimum.\n",
    "\n",
    "**Further discussion**: [Benchmarking Global Optimizers](https://fguvenendotcom.files.wordpress.com/2019/09/agk2019-september-nber-submit.pdf) ([code](https://github.com/serdarozkan/TikTak#tiktak))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the **constrained problem**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\min_{x_1,x_2,x_3,x_4} x_1x_4(x_1+x_2+x_3) + x_3\n",
    "$$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1x_2x_3x_4 &\\geq 25 \\\\\n",
    "x_1^2+x_2^2+x_3^2+x_4^2 &= 40 \\\\\n",
    "1 \\leq x_1,x_2,x_3,x_4 &\\leq 5\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define **objective** and **constraints**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _objective(x1,x2,x3,x4):\n",
    "    return x1*x4*(x1+x2+x3)+x3\n",
    "\n",
    "def objective(x):\n",
    "    return _objective(x[0],x[1],x[2],x[3])\n",
    "\n",
    "def ineq_constraint(x):\n",
    "    return x[0]*x[1]*x[2]*x[3]-25.0 # violated if negative\n",
    "\n",
    "def eq_constraint(x):\n",
    "    sum_eq = 40.0\n",
    "    for i in range(4):\n",
    "        sum_eq = sum_eq - x[i]**2\n",
    "    return sum_eq # must equal zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. setup\n",
    "bound = (1.0,5.0)\n",
    "bounds = (bound, bound, bound, bound)\n",
    "ineq_con = {'type': 'ineq', 'fun': ineq_constraint} \n",
    "eq_con = {'type': 'eq', 'fun': eq_constraint}\n",
    "\n",
    "# b. call optimizer\n",
    "x0 = (40**(1/8),40**(1/8),40**(1/8),40**(1/8)) # fit the equality constraint\n",
    "result = optimize.minimize(objective,x0,\n",
    "                             method='SLSQP',\n",
    "                             bounds=bounds,\n",
    "                             constraints=[ineq_con,eq_con],\n",
    "                             options={'disp':True})\n",
    "\n",
    "print('\\nx = ',result.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternative:** Extend the **objective function with a penalty term**, where guesses outside the allowed bounds and constraints are projected into the allowed region, but a (large) penalty term is added to discourage this. Solve this problem with an unconstrained solver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Economic application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following **consumption-saving problem**:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V(a_0) &= \\max_{c_1,c_2,c_3} \\frac{c_{1}^{1-\\rho}}{1-\\rho} + \\beta \\frac{c_{2}^{1-\\rho}}{1-\\rho} + \\beta^2\\frac{c_{3}^{1-\\rho}}{1-\\rho} + \\beta^2\\nu\\frac{(a_{3}+\\kappa)^{1-\\rho}}{1-\\rho}\\\\\n",
    "\\text{s.t.} \\\\\n",
    "&\\text{s.t.}&\\\\\n",
    "m_1 &= (1+r)a_0 + y_1\\\\\n",
    "a_1 &= m_1-c_1\\\\\n",
    "m_2 &= (1+r)a_1 + y_2\\\\\n",
    "a_2 &= m_2-c_2\\\\\n",
    "m_3 &= (1+r)a_2 + y_3\\\\\n",
    "a_3 &= m_3-c_3\\\\\n",
    "c_1,c_2,c_3 &\\geq 0\\\\\n",
    "a_1,a_2,a_3 &\\geq 0\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "* $m_t$ is cash-on-hand in period $t\\in\\{1,2,\\dots,T\\}$\n",
    "* $c_t$ is consumption $t$\n",
    "* $a_t$ is end-of-period assets and income in period $t$\n",
    "* ${y_t}$ is income in period $t$\n",
    "* $\\beta > 0$ is the discount factor\n",
    "* $r > -1$ is the interest rate \n",
    "* $\\rho > 1$ is the CRRA coefficient\n",
    "* $\\nu > 0 $ is the strength of the bequest motive\n",
    "* $\\kappa > 0$ is the degree of luxuriousness in the bequest motive  \n",
    "* $a_t\\geq0$ is a no-borrowing constraint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Guide to solve such problem:**\n",
    "\n",
    "1. Setup parameters\n",
    "2. Formulate objective function\n",
    "3. Determine how to handle constraints\n",
    "4. Call optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = SimpleNamespace()\n",
    "par.a0 = 0.5\n",
    "par.beta = 0.94\n",
    "par.r = 0.04\n",
    "par.rho = 8\n",
    "par.kappa = 0.5\n",
    "par.nu = 0.1\n",
    "par.y = [1,2,4]\n",
    "par.T = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objetive function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj(c,par,full_return=False):\n",
    "    \n",
    "    # objective function with penalty term\n",
    "    \n",
    "    # a. allocate\n",
    "    a = np.zeros(par.T) # end-of-period assets\n",
    "    m = np.zeros(par.T) # cash-on-hand\n",
    "    cb = np.zeros(par.T) # bounded\n",
    "    \n",
    "    # b. bound consumption and penalty\n",
    "    penalty = 0.0\n",
    "    for t in range(par.T):\n",
    "        \n",
    "        # i. lagged assets\n",
    "        a_lag = a[t-1] if t > 0 else par.a0\n",
    "        \n",
    "        # ii. cash-on-hand\n",
    "        m[t] = (1+par.r)*a_lag + par.y[t]\n",
    "        \n",
    "        # ii. bounded consumption\n",
    "        if c[t] < 0:\n",
    "            penalty += 10_000*np.abs(c[t]-0.0)\n",
    "            cb[t] = 0\n",
    "        elif c[t] > m[t]:\n",
    "            penalty += 10_000*np.abs(c[t]-m[t])\n",
    "            cb[t] = m[t]\n",
    "        else:\n",
    "            cb[t] = c[t]\n",
    "        \n",
    "        # d. end-of-period assets \n",
    "        a[t] = m[t] - cb[t]\n",
    "            \n",
    "    # c. utility\n",
    "    utility = 0.0\n",
    "    \n",
    "    # i. consumption\n",
    "    for t in range(par.T):\n",
    "        utility += par.beta**t*(cb[t]**(1-par.rho))/(1-par.rho)\n",
    "    \n",
    "    # ii. bequest\n",
    "    utility += par.beta**(par.T-1)*par.nu*(a[-1]+par.kappa)**(1-par.rho)/(1-par.rho)\n",
    "        \n",
    "    # d. return negative utility + penalty\n",
    "    if full_return:\n",
    "        return utility,m,a\n",
    "    else:\n",
    "        return -utility + penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solve:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(par):\n",
    "    \n",
    "    # a. initial geuss\n",
    "    x0 = [par.a0/par.T,par.a0/par.T,par.a0/par.T]\n",
    "    \n",
    "    # b. solve\n",
    "    results = optimize.minimize(obj,x0,args=(par,),method='nelder-mead')\n",
    "    assert results.success\n",
    "    print(f'solved in {results.nit} iteratoons [{results.nfev} function evaluations]')\n",
    "    \n",
    "    # c. details\n",
    "    c = results.x\n",
    "    utility,m,a = obj(c,par,full_return=True)\n",
    "    print(f't = 0: a = {par.a0:.4f}')\n",
    "    for t in range(par.T):\n",
    "        print(f't = {t+1}: y = {par.y[t]:.4f}, m = {m[t]:.4f}, c = {c[t]:.4f}, a = {a[t]:.4f}')    \n",
    "    print(f'utility = {utility:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve(par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happens if the income path is reversed?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par.y = list(reversed(par.y))\n",
    "solve(par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question I:** Could we easily allowing for borrowing, i.e.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V(a_0) &= \\max_{c_1,c_2,c_3} \\frac{c_{1}^{1-\\rho}}{1-\\rho} + \\beta \\frac{c_{2}^{1-\\rho}}{1-\\rho} + \\beta^2\\frac{c_{3}^{1-\\rho}}{1-\\rho} + \\beta^2\\nu\\frac{(a_{3}+\\kappa)^{1-\\rho}}{1-\\rho}\\\\\n",
    "\\text{s.t.} \\\\\n",
    "&\\text{s.t.}&\\\\\n",
    "m_1 &= (1+r)a_0 + y_1\\\\\n",
    "a_1 &= m_1-c_1\\\\\n",
    "m_2 &= (1+r)a_1 + y_2\\\\\n",
    "a_2 &= m_2-c_2\\\\\n",
    "m_3 &= (1+r)a_2 + y_3\\\\\n",
    "a_3 &= m_3-c_3\\\\\n",
    "c_1,c_2,c_3 &\\geq 0\\\\\n",
    "a_1,a_2 &\\geq -\\lambda\\\\\n",
    "a_3 &\\geq 0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question II:** Could we easily extend the problem to more periods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "V(a_0) &= \\max_{c_1,c_2,\\dots c_T} \\sum_{t=1}^T \\beta^{t-1} \\frac{c_{t}^{1-\\rho}}{1-\\rho} + \\beta^{T+1}\\nu\\frac{(a_{T}+\\kappa)^{1-\\rho}}{1-\\rho}\\\\\n",
    "\\text{s.t.} \\\\\n",
    "&\\text{s.t.}&\\\\\n",
    "m_t &= (1+r)a_{t-1} + y_t\\\\\n",
    "c_t &\\geq 0\\\\\n",
    "a_t &\\geq 0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Follow-up question:** What is the problem for $T \\rightarrow \\infty$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intermezzo:** To consider dynamic optimization problems, we need to think about interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inputs:**\n",
    "\n",
    "1. Sorted vector of known points (grid vector), $G = \\{G_i\\}_{i=0}^{n-1}$\n",
    "2. Vector of known values (at these points), $F = \\{F_i = f(G_i)\\}_{i=0}^{n-1}$\n",
    "3. A new point, `x`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm:** `linear_interpolate()`\n",
    "1. Determine `i`  such that\n",
    "\n",
    "$$\n",
    "G_i \\leq x < G_{i+1}\n",
    "$$\n",
    "\n",
    "2. Compute interpolated value by\n",
    "\n",
    "$$\n",
    "y =  F_{i} + \\frac{F_{i+1}-F_{i}}{G_{i+1}-G_{i}}(x-G_{i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extrapolation:**\n",
    "\n",
    "1. Below where $x < G_1$: \n",
    "\n",
    "$$\n",
    "y =  F_{0} + \\frac{F_{1}-F_{0}}{G_{1}-G_{0}}(x-G_{0})\n",
    "$$\n",
    "\n",
    "2. Above where $x > G_{n-2}$: \n",
    "\n",
    "$$\n",
    "y =  F_{n-2} + \\frac{F_{n-1}-F_{n-2}}{G_{n-1}-G_{n-2}}(x-G_{n-2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolate(G,F,x):\n",
    "    \"\"\" linear interpolation (and extrapolation)\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        G (np.ndarray): known points\n",
    "        F (np.ndarray): known values\n",
    "        x (float): point to be interpolated\n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "        y (float): intepolated value\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(G) == len(F)\n",
    "    n = len(G)\n",
    "    \n",
    "    # a. find index in known points\n",
    "    if x < G[1]: # exprapolation below\n",
    "        i = 0\n",
    "    elif x > G[-2]: # extrapolation above\n",
    "        i = n-2\n",
    "    else: # true interpolation\n",
    "        \n",
    "        # search\n",
    "        i = 0 \n",
    "        while x >= G[i+1] and i < n-1:\n",
    "            i += 1\n",
    "        \n",
    "        assert x >= G[i]\n",
    "        assert x < G[i+1]\n",
    "\n",
    "    # b. interpolate\n",
    "    diff_G = G[i+1]-G[i]\n",
    "    diff_F = F[i+1]-F[i]\n",
    "    slope = diff_F/diff_G\n",
    "    y = F[i] + slope*(x-G[i])\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following function and known points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: (x-3)**3 - 3*x**2 + 5*x\n",
    "\n",
    "G = np.linspace(-5,10,6)\n",
    "F = f(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple test:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [-2.3,4.1,7.5,9.1]:\n",
    "    true = f(x)\n",
    "    y = linear_interpolate(G,F,x)\n",
    "    print(f'x = {x:4.1f} -> true = {true:6.1f}, interpolated = {y:6.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scipy.interpolate:** Use the *RegularGridInterpolator*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. construct interpolation function\n",
    "interp_func = interpolate.RegularGridInterpolator([G],F,\n",
    "                                                  bounds_error=False,\n",
    "                                                  fill_value=None)\n",
    "\n",
    "# bounds_error=False and fill_value=None allow for extrapolation\n",
    "\n",
    "# b. interpolate\n",
    "grid = np.linspace(-7,12,500)\n",
    "interp_values = interp_func(grid)\n",
    "\n",
    "# c. evaluate true values\n",
    "true_values = f(grid)\n",
    "\n",
    "# d. plot true and interpolated values\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(G,F,'o',label='known points')\n",
    "ax.plot(grid,true_values,'-',lw=1,label='true function')\n",
    "ax.plot(grid,interp_values,'-',lw=1,label='interpolated values')\n",
    "ax.legend(loc='lower right',facecolor='white',frameon=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "1. Linear interpolation works best when the function does not curve too much.\n",
    "2. Extrapolation is much worse than interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiple dimensions:** Same principle, ``interpolate.RegularGridInterpolator([G1,G2,G3],F)``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic optimization problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following subject is hard. But also extremely useful. *If you master this, you can solve (almost) all economic models you meet on your way in life*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a **household** living in two periods.\n",
    "\n",
    "In the **second period** it gets utility from **consuming** and **leaving a bequest** (warm glow),\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{2}(m_{2})&= \\max_{c_{2}}\\frac{c_{2}^{1-\\rho}}{1-\\rho}+\\nu\\frac{(a_2+\\kappa)^{1-\\rho}}{1-\\rho}\\\\\n",
    "\\text{s.t.} \\\\\n",
    "a_2 &= m_2-c_2 \\\\\n",
    "a_2 &\\geq 0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where \n",
    "\n",
    "* $m_2$ is cash-on-hand \n",
    "* $c_2$ is consumption\n",
    "* $a_2$ is end-of-period assets \n",
    "* $\\rho > 1$ is the risk aversion coefficient\n",
    "* $\\nu > 0 $ is the strength of the bequest motive\n",
    "* $\\kappa > 0$ is the degree of luxuriousness in the bequest motive  \n",
    "* $a_2\\geq0$ ensures the household *cannot* die in debt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **value function** $v(m_2)$ measures the household's value of having $m_2$ at the beginning of period 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility(c,par):\n",
    "    return c**(1-par.rho)/(1-par.rho)\n",
    "\n",
    "def bequest(m,c,par):\n",
    "    return par.nu*(m-c+par.kappa)**(1-par.rho)/(1-par.rho)\n",
    "\n",
    "def v2(c2,m2,par):\n",
    "    return utility(c2,par) + bequest(m2,c2,par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **first period**, the household gets utility from consuming and takes into account that it will also live in the next-period, where it receives a stochastic income,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_1(m_1)&=\\max_{c_1}\\frac{c_{1}^{1-\\rho}}{1-\\rho}+\\beta\\mathbb{E}_{1}\\left[v_2(m_2)\\right]\\\\&\\text{s.t.}&\\\\\n",
    "a_1&=m_1-c_1\\\\\n",
    "m_2&= (1+r)(m_1-c_1)+y_2 \\\\\n",
    "y_{2}&= \\begin{cases}\n",
    "1-\\Delta & \\text{with prob. }0.5\\\\\n",
    "1+\\Delta & \\text{with prob. }0.5 \n",
    "\\end{cases}\\\\\n",
    "a_1&\\geq0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "* $m_1$ is cash-on-hand in period 1\n",
    "* $c_1$ is consumption in period 1\n",
    "* $a_1$ is end-of-period assets in period 1\n",
    "* $\\beta > 0$ is the discount factor\n",
    "* $\\mathbb{E}_1$ is the expectation operator conditional on information in period 1\n",
    "* $y_2$ is income in period 2\n",
    "* $\\Delta \\in (0,1)$ is the level of income risk (mean-preserving)\n",
    "* $r$ is the interest rate\n",
    "* $a_1\\geq0$ ensures the household *cannot* borrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v1(c1,m1,par,v2_interp):\n",
    "    \n",
    "    # a. v2 value, if low income\n",
    "    m2_low = (1+par.r)*(m1-c1) + 1-par.Delta\n",
    "    v2_low = v2_interp([m2_low])[0]\n",
    "    \n",
    "    # b. v2 value, if high income\n",
    "    m2_high = (1+par.r)*(m1-c1) + 1+par.Delta\n",
    "    v2_high = v2_interp([m2_high])[0]\n",
    "    \n",
    "    # c. expected v2 value\n",
    "    prob_low = 0.5\n",
    "    prob_high = 0.05\n",
    "    expected_v2 = prob_low*v2_low + prob_high*v2_high\n",
    "    \n",
    "    # d. total value\n",
    "    return utility(c1,par) + par.beta*expected_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve household problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose **parameters**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = SimpleNamespace()\n",
    "par.rho = 8\n",
    "par.kappa = 0.5\n",
    "par.nu = 0.1\n",
    "par.r = 0.04\n",
    "par.beta = 0.94\n",
    "par.Delta = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solve second period:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_period_2(par):\n",
    "\n",
    "    # a. grids\n",
    "    m2_vec = np.linspace(1e-4,5,500)\n",
    "    v2_vec = np.empty(500)\n",
    "    c2_vec = np.empty(500)\n",
    "\n",
    "    # b. solve for each m2 in grid\n",
    "    for i,m2 in enumerate(m2_vec):\n",
    "\n",
    "        # i. objective\n",
    "        obj = lambda x: -v2(x[0],m2,par)\n",
    "\n",
    "        # ii. initial value (consume half)\n",
    "        x0 = m2/2\n",
    "\n",
    "        # iii. optimizer\n",
    "        result = optimize.minimize(obj,[x0],method='L-BFGS-B',bounds=((1e-8,m2),))\n",
    "\n",
    "        # iv. save\n",
    "        v2_vec[i] = -result.fun\n",
    "        c2_vec[i] = result.x\n",
    "        \n",
    "    return m2_vec,v2_vec,c2_vec\n",
    "\n",
    "# solve\n",
    "m2_vec,v2_vec,c2_vec = solve_period_2(par)\n",
    "\n",
    "# illustration\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.plot(m2_vec,c2_vec)\n",
    "ax.set_xlabel('$m_2$')\n",
    "ax.set_ylabel('$c_2$')\n",
    "ax.set_title('consumption function in period 2')\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "ax.plot(m2_vec,v2_vec)\n",
    "ax.set_xlabel('$m_2$')\n",
    "ax.set_ylabel('$v_2$')\n",
    "ax.set_title('value function in period 2')\n",
    "ax.set_ylim([-40,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We now solve for the consumption function, rather than a specific optimum.\n",
    "\n",
    "**Question:** Why is there a kink in the consumption function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct interpolator:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2_interp = interpolate.RegularGridInterpolator([m2_vec], v2_vec,\n",
    "                                                bounds_error=False,fill_value=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solve first period:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_period_1(par,v2_interp):\n",
    "\n",
    "    # a. grids\n",
    "    m1_vec = np.linspace(1e-8,4,100)\n",
    "    v1_vec = np.empty(100)\n",
    "    c1_vec = np.empty(100)\n",
    "    \n",
    "    # b. solve for each m1 in grid\n",
    "    for i,m1 in enumerate(m1_vec):\n",
    "        \n",
    "        # i. objective\n",
    "        obj = lambda x: -v1(x[0],m1,par,v2_interp)\n",
    "        \n",
    "        # ii. initial guess (consume half)\n",
    "        x0 = m1/2\n",
    "        \n",
    "        # iii. optimize\n",
    "        result = optimize.minimize(obj,[x0],method='L-BFGS-B',bounds=((1e-12,m1),))\n",
    "        \n",
    "        # iv. save\n",
    "        v1_vec[i] = -result.fun\n",
    "        c1_vec[i] = result.x[0]\n",
    "     \n",
    "    return m1_vec,v1_vec,c1_vec\n",
    "\n",
    "# solve\n",
    "m1_vec,v1_vec,c1_vec = solve_period_1(par,v2_interp)\n",
    "\n",
    "# illustrate\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.plot(m1_vec,c1_vec)\n",
    "ax.set_xlabel('$m_1$')\n",
    "ax.set_ylabel('$c_1$')\n",
    "ax.set_title('consumption function in period 1')\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "ax.plot(m1_vec,v1_vec)\n",
    "ax.set_xlabel('$m_1$')\n",
    "ax.set_ylabel('$c_1$')\n",
    "ax.set_title('value function in period 1')\n",
    "ax.set_ylim([-40,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:** We can summarize what we have done in a single function doing:\n",
    "\n",
    "1. Solve period 2 (i.e. find $v_2(m_2)$ og $c_2(m_2)$)\n",
    "2. Construct interpolator of $v_2(m_2)$\n",
    "3. Solve period 1 (i.e. find $v_1(m_1)$ og $c_1(m_1)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(par):\n",
    "    \n",
    "    # a. solve period 2\n",
    "    m2_vec,v2_vec,c2_vec = solve_period_2(par)\n",
    "    \n",
    "    # b. construct interpolator\n",
    "    v2_interp = interpolate.RegularGridInterpolator([m2_vec], v2_vec,\n",
    "        bounds_error=False,fill_value=None)\n",
    "    \n",
    "    # b. solve period 1\n",
    "    m1_vec,v1_vec,c1_vec = solve_period_1(par,v2_interp)\n",
    "    \n",
    "    return m1_vec,c1_vec,m2_vec,c2_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot consumption function for various level of income risk**, i.e varios $\\Delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "_Delta = par.Delta\n",
    "for Delta in [0.05,0.15,0.25]:\n",
    "    par.Delta = Delta\n",
    "    m1_vec,c1_vec,m2_vec,c2_vec = solve(par)\n",
    "    ax.plot(m1_vec,c1_vec,label=f'$\\Delta = {Delta}$')\n",
    "\n",
    "# reset\n",
    "par.Delta = _Delta\n",
    "ax.legend(loc='lower right',facecolor='white',frameon=True)\n",
    "ax.set_xlabel('$m_1$')\n",
    "ax.set_ylabel('$c_1$')\n",
    "ax.set_title('value function in period 1')\n",
    "ax.set_xlim([0,2])\n",
    "ax.set_ylim([0,1.5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main takeaway:** The household lower its consumption when risk increases (such as in a recession). This is called **precautionary saving**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Solve and construct interpolators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Delta = 0.5\n",
    "m1_vec,c1_vec,m2_vec,c2_vec = solve(par)\n",
    "\n",
    "c1_interp = interpolate.RegularGridInterpolator([m1_vec], c1_vec,\n",
    "                                                bounds_error=False,fill_value=None)\n",
    "\n",
    "c2_interp = interpolate.RegularGridInterpolator([m2_vec], c2_vec,\n",
    "                                                bounds_error=False,fill_value=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Step 2:** Draw initail distribution of $m_1$ and simulate forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. draw initial m1\n",
    "simN = 10000\n",
    "sim_m1 = np.fmax(np.random.normal(1,0.1,size=simN),0) # \"randomly\" chosen distribution\n",
    "\n",
    "# b. period 1\n",
    "sim_c1 = c1_interp(sim_m1)\n",
    "sim_a1 = sim_m1-sim_c1\n",
    "\n",
    "# c. transition to period 2 with random draw\n",
    "sim_m2 = (1+par.r)*sim_a1+np.random.choice([0.5,1.5],p=[0.5,0.5])\n",
    "\n",
    "# d. period 2\n",
    "sim_c2 = c2_interp(sim_m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Plot distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "ax.hist(sim_c1,bins=100,label='period 1')\n",
    "ax.hist(sim_c2,bins=100,label='period 2')\n",
    "\n",
    "ax.legend(loc='lower right',facecolor='white',frameon=True)\n",
    "ax.set_xlabel('$c_t$')\n",
    "ax.set_ylabel('freq.')\n",
    "ax.set_title('consumption');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** You can now solve models with complex heterogeneity and uncertainty, and simulate the implied dynamics. By introducing various policies you can quantify their effect not just for the average, but for the full distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This lecture:**\n",
    "\n",
    "1. Solving multidimensional optimization problems with and without gradients (and hessians)\n",
    "2. Using multistart to alleviate problems with local minima (due to non-convexities)\n",
    "3. Using penalty terms to solve constrained optimization problems \n",
    "4. Linear interpolation between known points\n",
    "5. Solving dynamic optimization problems backwards period-by-period "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dynamic optimization:** Extremely useful technique. Can handle multiple periods, multiple states and choices, more shocks etc. You can solve general equilibrium models where the households solve such problems.\n",
    "\n",
    "**Need more dynamic optimization?** [Mini-Course on Dynamic Programming](https://github.com/NumEconCopenhagen/ConsumptionSavingNotebooks/tree/master/00.%20DynamicProgramming#mini-course-in-dynamic-programming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next lecture:** Canonical Economic Models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
