{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic data analysis\n",
    "\n",
    "You will learn how to **combine** (join and concatenate) datasets, **download online datasets** (through an API), and use **split-apply-combine** to calculate group-level statistics and make group-level plots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First** Let's start with a [survey](https://forms.office.com/Pages/ResponsePage.aspx?id=kX-So6HNlkaviYyfHO_6kckJrnVYqJlJgGf8Jm3FvY9UNVJVQk5IRzhBUFoxRVpZNkdHOE1GSVhTQiQlQCN0PWcu) on your background in data wrangling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to install the DST api-data reader, the pandas_datareader and the matplotlib_venn module. Uncomment the following cells and run to install.  \n",
    "The ! in front of each command indicates that this is a system command that may as well have been executed in the terminal/command prompt of your computer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DST API wrapper\n",
    "!pip install git+https://github.com/elben10/pydst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A wrapper for multiple APIs with a pandas interface\n",
    "!pip install pandas-datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Venn diagrams\n",
    "!pip install matplotlib-venn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import pandas_datareader # install with `pip install pandas-datareader`\n",
    "import pydst # install with `pip install git+https://github.com/elben10/pydst`\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "from matplotlib_venn import venn2 # `pip install matplotlib-venn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining datasets (merging and concatenating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When **combining datasets** there are a few crucial concepts: \n",
    "\n",
    "1. **Concatenate (append)**: \"stack\" rows (observations) on top of each other. This works if the datasets have the same columns (variables).\n",
    "2. **Merge**: the two datasets have different variables, but may or may not have the same observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **different kinds of merges** depending on which observations you want to keep:\n",
    "\n",
    "1. **Outer join (one-to-one)** Keep observations which are in *either* or in *both* datasets.\n",
    "2. **Inner join (one-to-one)** Keep observations which are in *both* datasets. \n",
    "3. **Left join (many-to-one)** Keep observations which are in the *left* dataset or in *both* datasets. \n",
    "\n",
    "Keeping observations which are not in both datasets will result in **missing values** for the variables comming from the dataset, where the observation does not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empl = pd.read_csv('../07/data/RAS200_long.csv') # .. -> means one folder up\n",
    "inc = pd.read_csv('../07/data/INDKP107_long.csv')\n",
    "area = pd.read_csv('../07/data/area.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating datasets\n",
    "\n",
    "Suppose we have two datasets that have the same variables and we just want to concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empl.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = empl.shape[0]\n",
    "\n",
    "A = empl.loc[empl.index < N/2,:] # first half of observations\n",
    "B = empl.loc[empl.index >= N/2,:] # second half of observations\n",
    "\n",
    "print(f'A has shape {A.shape} ')\n",
    "print(f'B has shape {B.shape} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concatenation** is done using the command `pd.concat([df1, df2])`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = pd.concat([A,B])\n",
    "print(f'C has shape {C.shape} (same as the original empl, {empl.shape})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two datasets with **different variables**: `empl` and `inc`. \n",
    "\n",
    "**Central command:** `pd.merge(empl, inc, on=[municipalitiy, year], how=METHOD)`. \n",
    "\n",
    "1. The keyword `on` specifies the **merge key(s)**. They uniquely identify observations in both datasets (for sure in at least one of them).  \n",
    "\n",
    "2. The keyword `how` specifies the **merge method** (taking values such as `'outer'`, `'inner'`, or `'left'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Look at datasets:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Years in empl: {empl.year.unique()}')\n",
    "print(f'Municipalities in empl = {len(empl.municipality.unique())}')\n",
    "print(f'Years in inc: {inc.year.unique()}')\n",
    "print(f'Municipalities in inc = {len(inc.municipality.unique())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find differences:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_y = [y for y in inc.year.unique() if y not in empl.year.unique()] \n",
    "print(f'years in inc data, but not in empl data: {diff_y}')\n",
    "\n",
    "diff_m = [m for m in empl.municipality.unique() if m not in inc.municipality.unique()] \n",
    "print(f'municipalities in empl data, but not in inc data: {diff_m}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** `inc` has more years than `empl`, but `empl` has one municipality that is not in `inc`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "v = venn2(subsets = (4, 4, 10), set_labels = ('empl', 'inc'))\n",
    "v.get_label_by_id('100').set_text('Cristians√∏')\n",
    "v.get_label_by_id('010').set_text('2004-07' )\n",
    "v.get_label_by_id('110').set_text('common observations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer join: union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "v = venn2(subsets = (4, 4, 10), set_labels = ('empl', 'inc'))\n",
    "v.get_label_by_id('100').set_text('included')\n",
    "v.get_label_by_id('010').set_text('included')\n",
    "v.get_label_by_id('110').set_text('included')\n",
    "plt.title('outer join')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer = pd.merge(empl,inc,on=['municipality','year'],how='outer')\n",
    "\n",
    "print(f'Number of municipalities = {len(outer.municipality.unique())}')\n",
    "print(f'Number of years = {len(outer.year.unique())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the **outer join** includes rows that exist in either dataframe and therefore includes missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = (outer.year.isin(diff_y)) | (outer.municipality.isin(diff_m))\n",
    "outer.loc[I, :].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "v = venn2(subsets = (4, 4, 10), set_labels = ('empl', 'inc'))\n",
    "v.get_label_by_id('100').set_text('dropped'); v.get_patch_by_id('100').set_alpha(0.05)\n",
    "v.get_label_by_id('010').set_text('dropped'); v.get_patch_by_id('010').set_alpha(0.05)\n",
    "v.get_label_by_id('110').set_text('included')\n",
    "plt.title('inner join')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner = pd.merge(empl,inc,how='inner',on=['municipality','year'])\n",
    "\n",
    "print(f'Number of municipalities = {len(inner.municipality.unique())}')\n",
    "print(f'Number of years          = {len(inner.year.unique())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the **inner join** does not contain any rows that are not in both datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = (inner.year.isin(diff_y)) | (inner.municipality.isin(diff_m))\n",
    "inner.loc[I, :].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my work, I most frequently use the **left join**. It is also known as a *many-to-one* join. \n",
    "\n",
    "* **Left dataset:** `inner` many observations of a given municipality (one per year),\n",
    "* **Right dataset:** `area` at most one observation per municipality and new variable (km2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_with_area = pd.merge(inner, area, on='municipality', how='left')\n",
    "inner_with_area.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'inner has shape {inner.shape}')\n",
    "print(f'area has shape {area.shape}')\n",
    "print(f'merge result has shape {inner_with_area.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "v = venn2(subsets = (4, 4, 10), set_labels = ('inner', 'area'))\n",
    "v.get_label_by_id('100').set_text('included:\\n no km2'); \n",
    "v.get_label_by_id('010').set_text('dropped'); v.get_patch_by_id('010').set_alpha(0.05)\n",
    "v.get_label_by_id('110').set_text('included:\\n with km2')\n",
    "plt.title('left join')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intermezzo:** Finding the non-overlapping observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_area = [m for m in inner.municipality.unique() if m not in area.municipality.unique()]\n",
    "not_in_inner = [m for m in area.municipality.unique() if m not in inner.municipality.unique()]\n",
    "\n",
    "print(f'There are {len(not_in_area)} municipalities in inner that are not in area. They are:')\n",
    "print(not_in_area)\n",
    "print('')\n",
    "\n",
    "print(f'There is {len(not_in_inner)} municipalities in area that are not in inner. They are:')\n",
    "print(not_in_inner)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check that km2 is never missing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_with_area.km2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative function for left joins: `df.join()` which uses the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a left join function `df.join()`, we must first set the **index**. Technically, we do not need this, but if you ever need to join on more than one variable, `df.join()` requires you to work with indices so we might as well learn it now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner.set_index(['municipality', 'year'], inplace=True)\n",
    "area.set_index('municipality', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner.head()\n",
    "final = inner.join(area)\n",
    "print(f'final has shape: {final.shape}')\n",
    "final.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other programming languages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL** (including SAS *proc sql*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL is one of the most powerful database languages and many other programming languages embed a version of it. For example, SAS has the `proc SQL`, where you can use SQL syntax. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL is written in statements such as \n",
    "\n",
    "* **left join**   `select * from empl left join inc on empl.municipality = inc.municipality and empl.year = inc.year`\n",
    "* **outer join** `select * from empl full outer join inc on empl.municipality = inc.municipality and empl.year = inc.year`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Stata, the command `merge` nests many of the commands mentioned above. You specify `merge 1:1`  for a one-to-one merge or `merge m:1`  or `merge 1:m`  for many-to-one or one-to-many merges, and you do not use `merge m:m` (until you are quite advanced). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching data using an API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API stands for **Application Programming Interface**. An API is an interface through which we can directly ask for and **receive data from an online source**. We will be using packages for this and will not look at what is going on underneath. \n",
    "\n",
    "1. We use `pandas_datareader` to access many common **international online data** sources (install with `pip install pandas-datareader`)\n",
    "2. For **Statistics Denmark**, Jakob Elben has written the `pydst` package (install with `pip install git+https://github.com/elben10/pydst`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching data from an API requires an **internet connection** and works directly without saving data to your harddisk (unless you ask Python to do so afterwards). You can use it to automate tasks such as fetching the most recent data, doing some calculations and outputting it in the same manner. This can be useful e.g. for quarterly reports. Remember to save the data on your computer if you really need for later though. The admins of the data may turn off the water.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros:** Automatic; smart; everything is done from Python (so no need to remember steps in between). \n",
    "\n",
    "**Cons:** The connection can be slow or drop out, which may lead to errors. If e.g. 100 students simultaneously fetch data (during, say, a lecture), the host server may not be able to service all the requests and may drop out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The raw output data from an API could look like this: https://stats.oecd.org/SDMX-JSON/data/NAAG. It is a log list of non-human-readable gobledygook in the so-called \"JSON\" format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from Denmark Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup:**  \n",
    "Create an dst api **object** that will allow us to interact with the DST server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dst = pydst.Dst(lang='en') # setup data loader with the langauge 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the Dst variable?\n",
    "print(type(Dst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from DST are organized into: \n",
    "\n",
    "1. **Subjects:** indexed by numbers. Use `Dst.get_subjects()` to see the list. \n",
    "2. **Tables:** with names like \"INDKP107\". Use `Dst.get_tables(subjects=['X'])` to see all tables in a subject. \n",
    "\n",
    "**Data is extracted**  with `Dst.get_data(table_id = 'NAME', variables = DICT)`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subjects:** With `Dst.get_subjects()` we can list all subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dst.get_subjects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tables:** With `get_tables()`, we can list all tables under a subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = Dst.get_tables(subjects=['2'])\n",
    "print(type(tables))\n",
    "display(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variable in a dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[tables.id == 'INDKP107']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indk_vars = Dst.get_variables(table_id='INDKP107')\n",
    "indk_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to know the available levels of each conditioning variable that we may subset by. Use a loop to print out those levels. \n",
    "\n",
    "\n",
    "**Values of variable in a dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indk_vars = Dst.get_variables(table_id='INDKP107')\n",
    "\n",
    "for id in ['ENHED','KOEN','UDDNIV','INDKOMSTTYPE']:\n",
    "    print(id)\n",
    "    values = indk_vars.loc[indk_vars.id == id,['values']].values[0,0]\n",
    "    for value in values:      \n",
    "        print(f' id = {value[\"id\"]}, text = {value[\"text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few to select from. Need to use a dictionary to specify the desired subset of data. Note: a **\\*** indicates that you want all levels. For example, we are subsetting all periods below.  \n",
    "\n",
    "**Get data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {'OMR√ÖDE':['*'],'ENHED':['110', '116'],'KOEN':['M','K'],'TID':['*'],'UDDNIV':['65'],'INDKOMSTTYPE':['100']}\n",
    "inc_api = Dst.get_data(table_id = 'INDKP107', variables=variables)\n",
    "inc_api.sort_values(by=['OMR√ÖDE', 'TID', 'KOEN'], inplace=True)\n",
    "inc_api.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. now you have a data set ready for cleaning and renaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FRED (Federal Reserve Economic Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GDP data** for the US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need first to encode dates in a python friendly to specify the length of the desired time period. \n",
    "# Use the datetime module - it is the general way to handle dates in python. \n",
    "start = datetime.datetime(2005,1,1)\n",
    "end = datetime.datetime(2017,1,1)\n",
    "timespan = end - start # We can investigate the precise time span by just subtracting to time variables.\n",
    "print('total number of days:', timespan.days) # The timespan object has a days attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the FRED api using pandas_datareader \n",
    "gdp = pandas_datareader.data.DataReader('GDP', 'fred', start, end)\n",
    "gdp.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding data:**\n",
    "\n",
    "1. go to https://fred.stlouisfed.org \n",
    "2. search for data in main bar, e.g. employment and unemployment\n",
    "3. click the first links\n",
    "4. table name is next to header "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to pull down data on aggregate employment (PAYEMS) and unemployment (UNEMPLOY) levels and then plot the development. \n",
    "\n",
    "**Pulling the data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime(1939,1,1)\n",
    "end = datetime.datetime(2021,12,1)\n",
    "\n",
    "# We can pull from multiple sources in one go. Just combine them in a list.\n",
    "empl_us = pandas_datareader.data.DataReader(['PAYEMS', 'UNEMPLOY'], 'fred', start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "# Now we are just plotting directly from the pandas dataframe. Still using matplotlib under the hood.\n",
    "empl_us.plot(ax=ax)\n",
    "\n",
    "ax.legend(frameon=True)\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('employment (US)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World Bank indicators: `wb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding data:**\n",
    "\n",
    "1. go to https://data.worldbank.org/indicator/\n",
    "2. search for GDP \n",
    "3. variable name (\"NY.GDP.PCAP.KD\") is in the URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pull GDP numbers:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need a different module than in the FRED case\n",
    "from pandas_datareader import wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_gdp = wb.download(indicator='NY.GDP.PCAP.KD', country=['SE','DK','NO'], start=1990, end=2017)\n",
    "\n",
    "wb_gdp = wb_gdp.rename(columns = {'NY.GDP.PCAP.KD':'GDP'})\n",
    "wb_gdp = wb_gdp.reset_index()\n",
    "wb_gdp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_gdp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problems:** \n",
    "* It turns out that the dataframe has stored the variable *year* as an \"object\", meaning in practice that it is a string. This must be converted to an int, as we want to use it as a number. \n",
    "* *country* is in fact a text variable, so it is acceptable to have it as an object type. But pandas has implemented a string type on its own. It is called 'string', while the text type of object that you normally encounter is of type 'str'. Yes, confusing!!  But you want to get it right, because an object variable can also contain numbers in addition to text. Which is bad. \n",
    "* Fortunately, GDP is a float (i.e. a number). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_gdp.year = wb_gdp.year.astype(int) # convert year\n",
    "wb_gdp.country = wb_gdp.country.astype('string') # convert country to the special pandas string type\n",
    "wb_gdp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fetch employment-to-population ratio:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_empl = wb.download(indicator='SL.EMP.TOTL.SP.ZS', country=['SE','DK','NO'], start=1990, end=2017) # don't need the special datetime here.\n",
    "wb_empl.rename(columns = {'SL.EMP.TOTL.SP.ZS':'employment_to_pop'}, inplace=True) # Better col name\n",
    "wb_empl.reset_index(inplace = True)\n",
    "wb_empl.year = wb_empl.year.astype(int)\n",
    "wb_empl.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wb = pd.merge(wb_gdp, wb_empl, how = 'outer', on = ['country','year']);\n",
    "wb.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split-apply-combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most useful skills to learn is **the split-apply-combine process**. For example, we may want to compute the average employment rate within a municipality over time and calculate whether the employment rate in each year is above or below the average. We calculate this variable using a split-apply-combine procedure: \n",
    "\n",
    "1. **split**: divide the dataset into units (one for each municipality)\n",
    "2. **apply**: compute the average employment rate for each unit\n",
    "3. **combine**: merge this new variable back onto the original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empl.rename(columns={'empl':'e'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple, unconditional transformation of data. Works because of broadcasting. \n",
    "empl['empl_demean'] = empl.e - empl.e.mean()\n",
    "empl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empl = empl.sort_values(['municipality','year']) # sort by first municipality then year\n",
    "empl.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **groupby** to calculate **within means**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empl.groupby(['municipality'])['e'].mean().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom functions** the apply part can be specified by using the `lambda` notation. \n",
    "\n",
    "**Warning:** `lambda` implementations will often be a pretty slow alternative to *vectorized* operations. More on that later. \n",
    "\n",
    "An example with average change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a lambda function to applied down rows of a column in blocks defined by the groupby. \n",
    "avg_first_diff = lambda x: x.diff(1).mean() # A pd.Series has a function diff that does the job.\n",
    "\n",
    "# Apply the lambda and print head of output\n",
    "empl.groupby('municipality')['e'].apply(avg_first_diff).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also define our lambda with a numpy implementation. \n",
    "avg_first_diff = lambda x: np.mean(x[1:]-x[:-1])\n",
    "\n",
    "# Need the extra lambda function to retrieve values (aka a numpy array) of e for the avg_first_diff.\n",
    "empl.groupby('municipality')['e'].apply(lambda x: avg_first_diff(x.values)).head(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot statistics**: Dispersion in employment rate across Danish municipalities over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "empl.groupby('year')['e'].std().plot(ax=ax,style='-o')\n",
    "\n",
    "ax.set_ylabel('std. dev.')\n",
    "ax.set_title('std. dev. across municipalities in the employment rate');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split-Apply-Combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Calculate within municipality difference to mean employment rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start** by splitting, applying and combining **manually:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Split**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_grouped = empl.groupby('municipality')['e']\n",
    "\n",
    "# The e_grouped object is not ready for inspection\n",
    "print(e_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Apply:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_mean = e_grouped.mean() # mean employment rate\n",
    "e_mean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change name of series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_mean.name = 'e_mean' # necessary for join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Combine:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empl_ = empl.set_index('municipality').join(e_mean, how='left')\n",
    "empl_['e_demean'] = empl_.e - empl_.e_mean\n",
    "empl_.xs('Copenhagen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "municipalities = ['Copenhagen','Roskilde','Lejre']\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "# Here we use the fact that the index has multiple levels (years) for an elegant loop\n",
    "for m in municipalities:\n",
    "    empl_.xs(m).plot(x='year',y='e_demean',ax=ax,label=m)\n",
    "\n",
    "ax.legend(frameon=True)\n",
    "ax.set_ylabel('difference to mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the splitting and applying in one fell swoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with `agg()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agg:** The same value for all observations in a group.  \n",
    "We can use **lambdas** or **built-in** functions for the operation. ***Use built-in whenever you can!*** Here we use lambda for exposition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empl_ = empl.copy()\n",
    "\n",
    "# a. Good use: a built-in function for mean rather than lambda.\n",
    "e_mean = empl_.groupby('municipality')['e'].agg('mean')\n",
    "\n",
    "# Same result with a lambda\n",
    "#e_mean = empl_.groupby('municipality')['e'].agg(lambda x: x.mean())\n",
    "\n",
    "e_mean.name = 'e_mean'\n",
    "\n",
    "# b. combine\n",
    "empl_ = empl_.set_index('municipality').join(e_mean, how='left')\n",
    "empl_['diff'] = empl_.e - empl_.e_mean\n",
    "empl_.xs('Copenhagen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Same result!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Are there any dangers with the variable name 'diff'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty cumbersome though. Creating a new variable and then merging in separate step - we can do better with the tools in Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting, applying and combining all together\n",
    "### with - `apply()`  and `transform()` directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform:** In case you are dealing with multiple variables, `transform` will work on one variable/column at a time. In the case below, had we selected both *e* and *year* rather than just *e*, x.mean() would and could only have been applied to observations within one column at a time. `transform` has to return an array of size 1 or of the same size as the original column.  \n",
    "**Apply:** The set of columns passed to the `apply` function is considered to be a whole dataframe on its own. You can therefore make lambda functions that utilizes several columns of data in each operation. That is not possible with the `transform` function.  \n",
    "**More info:** you can read more about the differences between transform and `apply` [here](https://stackoverflow.com/questions/27517425/apply-vs-transform-on-a-group-object).    \n",
    "\n",
    "**Note** when you are dealing with selections of **only 1** variable, then `transform` and `apply` behave similarly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empl_ = empl.copy()\n",
    "empl_['e_demean'] = empl_.groupby('municipality')['e'].apply(lambda x:  x - x.mean())\n",
    "\n",
    "# In this case, you could have used apply as well\n",
    "#empl_[['e_demean']] = empl_.groupby('municipality')['e'].transform(lambda x: x - x.mean())\n",
    "\n",
    "empl_.set_index('municipality').xs('Copenhagen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing performance\n",
    "\n",
    "It is quite important for your own and other's productivity to implement effecient procedures when dealing with large datasets. The **apply** method (as well as the transform) essentially **loops over** the rows of a column when applying a **lambda** function. This may be much **slower** than needed if you for example end up calculating averages over the whole column or group many, many times (one per row) as in the case below. Using pandas functions **without lambdas** gets it right. Important to avoid such behavior with large data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "N = 300\n",
    "\n",
    "# a. Check performance with lambda function. Sooo slooow.. \n",
    "demean = lambda x: x - x.mean()\n",
    "tic = time.perf_counter()\n",
    "for i in range(N):\n",
    "    d1 = empl.groupby('municipality')['e'].transform(demean)\n",
    "toc = time.perf_counter()\n",
    "print(f'Performance with lambda function {toc-tic: 5.3f}')\n",
    "\n",
    "# b. Performance when relying on built-in pandas methods. It is not because we're using transform per se. \n",
    "# It's much faster, because mean is not calculated for each row in data and we're in Cython. \n",
    "tic = time.perf_counter()\n",
    "for i in range(N):\n",
    "    d2 = empl.e - empl.groupby('municipality')['e'].transform('mean') # Demean by subtracting grouped mean from e column.    \n",
    "toc = time.perf_counter()\n",
    "print(f'Performance with pandas vectorized: {toc-tic: 5.3f}')\n",
    "\n",
    "print('Check of consistency: ', np.all(d1==d2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that an explicit numpy implementation is faster than relying on pandas methods. The example with first differencing from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. The pandas series implementation\n",
    "avg_first_diff = lambda x: x.diff(1).mean()\n",
    "tic = time.perf_counter()\n",
    "for i in range(N):\n",
    "    d1 = empl.groupby('municipality')['e'].apply(avg_first_diff)\n",
    "toc = time.perf_counter()\n",
    "print(f'Performance with pandas: {toc-tic: 3.6f}')\n",
    "\n",
    "# b. The Numpy implementation\n",
    "avg_first_diff = lambda x: np.mean(x.values[1:]-x.values[:-1])\n",
    "tic = time.perf_counter()\n",
    "for i in range(N):\n",
    "    d2 = empl.groupby('municipality')['e'].apply(avg_first_diff)\n",
    "toc = time.perf_counter()\n",
    "print(f'Performance with numpy: {toc-tic: 3.6f}')\n",
    "print('Is d1 == d2:', np.all(d1==d2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Same result!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need more complex group by stuff? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional links\n",
    "* Do you have **missing values** in data? Check [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html)\n",
    "* About strings and the `object`type in pandas, [here](https://towardsdatascience.com/why-we-need-to-use-pandas-new-string-dtype-instead-of-object-for-textual-data-6fd419842e24).\n",
    "* Comparison of SQL statements and pandas group by [here](https://realpython.com/pandas-groupby/)\n",
    "* Optimizing pandas routines incl. `apply`, [here](https://realpython.com/fast-flexible-pandas/#pandas-apply). (*less technical*)\n",
    "* Stackoverflow [musings](https://stackoverflow.com/questions/54432583/when-should-i-not-want-to-use-pandas-apply-in-my-code?noredirect=1&lq=1) on optimal use of apply( ) and it's downsides. See also [this](https://stackoverflow.com/questions/38938318/why-apply-sometimes-isnt-faster-than-for-loop-in-pandas-dataframe). (*both pretty technical*)\n",
    "* About optimizing pandas with numpy and vectorization, [here](https://devopedia.org/optimizing-pandas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few examples of open access APIs\n",
    "\n",
    "As already demonstrated, you can pull data from DST using their API. Just to give a few examples of where else you may find open access to data by API: \n",
    "* Check out the [documentation for pandas_datareader](https://pandas-datareader.readthedocs.io/en/latest/remote_data.html). There is a bunch of economic data banks to access through that. \n",
    "* There is an API for [covid-19 data](https://pypi.org/project/covid19-data/) that draws on several sources. \n",
    "* The National Museum of Art (DK) gives access to their collection [by an API](https://www.smk.dk/article/smk-api/). \n",
    "* NASA has its own API. [Look here](https://api.nasa.gov/) for their documentation and [here](https://pypi.org/project/python-nasa-api/) for a Python wrapper. \n",
    "\n",
    "# Other sources\n",
    "* **A crazy large collection of APIs** found on [this Github repo](https://github.com/public-apis/public-apis). Stocks, government, map data, lol memes, anything..\n",
    "* Datasets behind [FiveThirtyEight articles](https://github.com/fivethirtyeight/data)  (youknow.. [this site](https://fivethirtyeight.com/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All built-in functions to aggregate data in Pandas\n",
    "\n",
    "You can use the functions in `apply()`, `transform()` and `agg()` by writing them out in a string. See above. Will normally be the fastest implementation. \n",
    "\n",
    "***Function***\t *Description*\n",
    "\n",
    "\n",
    "* **count:**\t Number of non-null observations\n",
    "* **sum:**\t Sum of values\n",
    "* **mean:**\t Mean of values\n",
    "* **mad:**\t Mean absolute deviation\n",
    "* **min:**\t Minimum\n",
    "* **max:**\t Maximum\n",
    "* **mode:**\t Mode\n",
    "* **abs:**\t Absolute Value\n",
    "* **prod:**\t Product of values\n",
    "* **std:**\t Unbiased standard deviation\n",
    "* **var:**\t Unbiased variance\n",
    "* **sem:**\t Unbiased standard error of the mean\n",
    "* **skew:**\t Unbiased skewness (3rd moment)\n",
    "* **kurt:**\t Unbiased kurtosis (4th moment)\n",
    "* **quantile:**\t Sample quantile (value at %)\n",
    "* **cumsum:**\t Cumulative sum\n",
    "* **cumprod:**\t Cumulative product\n",
    "* **cummax:**\t Cumulative maximum\n",
    "* **cummin:**\t Cumulative minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also your [DataCamp cheatsheet](https://www.datacamp.com/community/blog/python-pandas-cheat-sheet) for pandas for references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This lecture:** We have discussed\n",
    "\n",
    "1. Combining datasets (**merging** and concatenating)\n",
    "2. Fatching data using an **API** (DST, FRED, World Bank, etc.)\n",
    "3. **Split-apply-combine** (groupby, agg, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your work:** Before solving Problem Set 4 read through this notebook and play around with the code.\n",
    "\n",
    "**Project 1:** See the details in the projects folder of Lectures2021 or *Project 1: Data analysis* [here](https://numeconcopenhagen.netlify.com/exercises/).<br>\n",
    "**Deadline:** 11th of April."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next lecture:** Algorithms: Searching and sorting algorithms.  \n",
    "Remember **no lecture** Monday 5th of April! (it's a holiday)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
